{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtpSmyAmALfadEiIknGjuK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JadHay8/Computer-Vision-Market-Predictor/blob/main/model_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ],
      "metadata": {
        "id": "zmrH9gTJUD1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = [\"path/to/image1.png\", \"path/to/image2.png\", ...]\n",
        "labels = [0, 1, ...]  # 0 for decrease, 1 for increase\n"
      ],
      "metadata": {
        "id": "R6cdjIjUTn_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Images"
      ],
      "metadata": {
        "id": "SZIUWzuXUGlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Assuming you have a directory structure for training and validation\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    'data/train',\n",
        "    target_size=(150, 150),  # Resize images\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    'data/validation',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')"
      ],
      "metadata": {
        "id": "udZm12xfTsDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design the Neural Network Model"
      ],
      "metadata": {
        "id": "Z2PE4lZMULy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "7d2RPRo0T28h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "t2kW_5qDUQJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,  # Depends on your dataset size\n",
        "    epochs=15,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)  # Adjust based on your validation dataset size\n"
      ],
      "metadata": {
        "id": "atkKhsbiT4FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate and Use the Model"
      ],
      "metadata": {
        "id": "cuVdx80bUUlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load and preprocess a new image\n",
        "new_image_path = 'path/to/new/image.png'\n",
        "img = image.load_img(new_image_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255.  # Remember to normalize the image in the same way as the training data\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(img_tensor)\n",
        "if prediction[0] > 0.5:\n",
        "    print(\"Increase\")\n",
        "else:\n",
        "    print(\"Decrease\")\n"
      ],
      "metadata": {
        "id": "Fyr0LnhaT_0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Test without Data"
      ],
      "metadata": {
        "id": "RfcE9HPsUZSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Image"
      ],
      "metadata": {
        "id": "hqO6D3NWZdfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "  print(f\"User uploaded file {filename} with length {len(uploaded[filename])} bytes\")"
      ],
      "metadata": {
        "id": "-8-arWK8ZgFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "img_path = '/content/candlestick_chart.png'\n",
        "img = image.load_img(img_path, target_size=(150, 150))  # Resize image to match model expected input\n",
        "\n",
        "# Convert the image to a numpy array and normalize\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0) / 255.  # Normalize to 0-1 range\n"
      ],
      "metadata": {
        "id": "vbmuWZxFR4bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define a simple model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Assuming binary classification (up/down)\n",
        "])\n",
        "\n",
        "# Since we're not training the model, the weights are random\n"
      ],
      "metadata": {
        "id": "72IvYakASH7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(img_array)\n",
        "print(f\"Mock Prediction (Untrained): {prediction[0]}\")\n",
        "# This will just give a random output\n"
      ],
      "metadata": {
        "id": "cylv7HIYSKzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Process with Image Generation Rather Than Upload"
      ],
      "metadata": {
        "id": "KtY1wqJpYkZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and Save a Chart"
      ],
      "metadata": {
        "id": "h83aUTxnyRv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mplfinance\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import mplfinance as mpf\n",
        "\n",
        "# Download historical data for a stock (e.g., Apple)\n",
        "stock_symbol = \"AAPL\"\n",
        "stock_data = yf.download(stock_symbol, start=\"2020-01-01\", end=\"2022-01-01\")\n",
        "\n",
        "# Specify the savefig dictionary with filename and other parameters (optional)\n",
        "save_params = dict(fname='apple_candlestick_chart.png', dpi=100, pad_inches=0.25)\n",
        "\n",
        "# Plot and save the candlestick chart\n",
        "mpf.plot(stock_data, type='candle', style='charles', volume=True, savefig=save_params)\n",
        "\n",
        "\n",
        "mpf.plot(stock_data, type='candle', style='charles', volume=True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "5W9LixJsXrSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Image Just Created"
      ],
      "metadata": {
        "id": "cHJMz0ezlz3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Load the image\n",
        "img_path = '/content/apple_candlestick_chart.png'\n",
        "img = image.load_img(img_path, target_size=(150, 150))  # Resize image to match model expected input\n",
        "\n",
        "# Convert the image to a numpy array and normalize\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0) / 255.  # Normalize to 0-1 range\n",
        "\n",
        "# Define a simple model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Assuming binary classification (up/down)\n",
        "])\n",
        "\n",
        "# Since we're not training the model, the weights are random\n",
        "\n",
        "prediction = model.predict(img_array)\n",
        "print(f\"Mock Prediction (Untrained): {prediction[0]}\")\n",
        "# This will just give a random output"
      ],
      "metadata": {
        "id": "q6Moz6C9lues"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try With Generation of Multiple Charts for Multiple Tickers"
      ],
      "metadata": {
        "id": "JZYiRB2XY-Mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ignore This For Now"
      ],
      "metadata": {
        "id": "LbNfYgbfePim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mplfinance\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import mplfinance as mpf\n",
        "\n",
        "# List of stock symbols to download and generate candlestick charts for\n",
        "stock_symbols = [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"META\", \"TSLA\", \"NFLX\", \"NVDA\", \"BABA\", \"JPM\"]\n",
        "\n",
        "\n",
        "def download_and_generate_charts(stock_symbols, window_size=5):\n",
        "    labels = {}  # Dictionary to hold the labels for each stock symbol\n",
        "\n",
        "    for symbol in stock_symbols:\n",
        "        # Download historical data\n",
        "        data = yf.download(symbol, start=\"2020-01-01\", end=\"2020-03-01\")\n",
        "\n",
        "        for i in range(len(data) - window_size):\n",
        "            # Select data window\n",
        "            data_window = data.iloc[i:i+window_size]\n",
        "\n",
        "            # Define filename for the chart image\n",
        "            filename = f\"{symbol}_{i}.png\"\n",
        "\n",
        "            # Plot and save the candlestick chart\n",
        "            mpf.plot(data_window, type='candle', style='charles', volume=True,\n",
        "                     savefig=dict(fname=filename, dpi=100, pad_inches=0.25))\n",
        "\n",
        "            # Determine the label based on price movement\n",
        "            try:\n",
        "                last_day_close = data_window['Close'].iloc[-1]\n",
        "                next_day_open = data.iloc[i + window_size]['Open']\n",
        "                label = 1 if next_day_open > last_day_close else 0  # 1 for up, 0 for down\n",
        "                labels[filename] = label\n",
        "            except IndexError:  # Handle the case where there's no next day\n",
        "                # This might happen at the end of the dataset\n",
        "                labels[filename] = None\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Generate charts and labels\n",
        "labels = download_and_generate_charts(stock_symbols)\n",
        "# print(labels)\n",
        "\n",
        "\n",
        "\n",
        "# mpf.plot(stock_data, type='candle', style='charles', volume=True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "nxOO4IBwZD-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Method"
      ],
      "metadata": {
        "id": "DizmtBO_eTO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Images and Labels"
      ],
      "metadata": {
        "id": "wa73xgKOjOKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mplfinance\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import mplfinance as mpf\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# List of stock symbols to download and generate candlestick charts for\n",
        "stock_symbols = [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"META\", \"TSLA\", \"NFLX\", \"NVDA\", \"BABA\", \"JPM\"]\n",
        "\n",
        "\n",
        "def download_and_generate_charts(stock_symbols):\n",
        "    labels = {}  # Dictionary to hold the labels for each stock symbol\n",
        "    start_date = \"2020-01-01\"\n",
        "    end_date =\"2020-03-01\"\n",
        "\n",
        "    # Get next day\n",
        "    new_end = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=3)\n",
        "    new_end_date = new_end.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for symbol in stock_symbols:\n",
        "        # Download historical data\n",
        "        data = yf.download(symbol, start=start_date, end=end_date)\n",
        "\n",
        "        # Download subsequent data to see if it went up or not\n",
        "        subseq_data = yf.download(symbol, start=end_date, end = new_end_date)\n",
        "\n",
        "        # Define filename for the chart image\n",
        "        filename = f\"{symbol}.png\"\n",
        "\n",
        "        # Plot and save the candlestick chart\n",
        "        mpf.plot(data, type='candle', style='charles', volume=True,\n",
        "                savefig=dict(fname=filename, dpi=100, pad_inches=0.25))\n",
        "\n",
        "        # Determine the label based on price movement\n",
        "        try:\n",
        "            last_day_close = data['Close'].iloc[-1]\n",
        "            next_day_open = subseq_data.iloc[-1]['Close']\n",
        "\n",
        "            # 2 for increase, 0 for decrease, 1 for neither\n",
        "            if next_day_open > last_day_close:\n",
        "              label = 2\n",
        "            elif next_day_open < last_day_close:\n",
        "              label = 0\n",
        "            else:\n",
        "              label = 1\n",
        "\n",
        "            # print(f\"symbol: {symbol} \\t p1: {last_day_close} \\t p2: {next_day_open} \\t label: {label} \\n\")\n",
        "\n",
        "            labels[filename] = label\n",
        "        except IndexError:  # Handle the case where there's no next day\n",
        "            # This might happen at the end of the dataset\n",
        "            labels[filename] = None\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Generate charts and labels\n",
        "labels = download_and_generate_charts(stock_symbols)\n",
        "print(labels)\n",
        "\n",
        "\n",
        "\n",
        "# mpf.plot(stock_data, type='candle', style='charles', volume=True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "7GCfOArSeN8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create, Train, and Evaluate Model"
      ],
      "metadata": {
        "id": "IjCRhgkulcLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Define image dimensions\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# Updated model definition for 3-class classification with integer labels\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # Output layer for 3 classes\n",
        "])\n",
        "\n",
        "# Compile the model with sparse_categorical_crossentropy\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# List to store model performance metrics\n",
        "model_metrics = []\n",
        "\n",
        "# Loop through each image and its label\n",
        "for filename, label in labels.items():\n",
        "    try:\n",
        "        # Load image and preprocess\n",
        "        img = load_img(filename, target_size=(img_width, img_height))\n",
        "        img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "        img_array = np.expand_dims(img_array, axis=0)  # Adjust dimensions\n",
        "\n",
        "        # Use the same data for training and validation for simplicity\n",
        "        X_train, X_val = img_array, img_array\n",
        "        y_train, y_val = np.array([label]), np.array([label])\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "        # Evaluate the model\n",
        "        _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "        model_metrics.append((filename, accuracy))\n",
        "\n",
        "        print(f\"Model for {filename}: Accuracy = {accuracy}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "xy0gggkqlgUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes"
      ],
      "metadata": {
        "id": "VhK5wWk0WL3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code still uses the same image for both training and validation, which is not ideal for a realistic scenario. We'll want to have a more robust dataset and potentially use data augmentation to improve our model's generalization capability\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Also, for each stock symbol, we're essentially training a model on a single instance, which is highly insufficient. A more effective approach would involve ***collecting a large dataset of candlestick charts across various time frames and conditions for each stock, then splitting this dataset into training, validation, and test sets***. This way, the model can learn general patterns associated with price movements rather than memorizing the specific details of a few charts."
      ],
      "metadata": {
        "id": "2f27GNGIWNU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying with one stock symbol over many date ranges"
      ],
      "metadata": {
        "id": "jVDP5XOaLhHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mplfinance\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import mplfinance as mpf\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import random\n",
        "!python --version\n",
        "\n",
        "help(mpf.plot)\n",
        "\n",
        "\n",
        "# List of stock symbols to download and generate candlestick charts for\n",
        "stock_symbols = [\"SPY\"]\n",
        "p = True\n",
        "\n",
        "def download_and_generate_charts(stock_symbols, num_samples_per_stock=100):\n",
        "    global p\n",
        "    labels = {}  # array to hold the labels for each graph/file\n",
        "\n",
        "    # Define date range for data download (adjust as needed)\n",
        "    start_date_range = \"2022-01-01\"\n",
        "    end_date_range = \"2023-12-31\"\n",
        "\n",
        "    hourly_data = pd.DataFrame(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "\n",
        "    for symbol in stock_symbols:\n",
        "        for _ in range(num_samples_per_stock):\n",
        "            # Generate random start and end dates within the range\n",
        "            start_date, end_date = random_date(start_date_range, end_date_range)\n",
        "\n",
        "            # Download historical data\n",
        "            data = yf.download(symbol, start=start_date, end=end_date)\n",
        "            hourly_data = yf.Ticker(symbol).history(interval='60m', start=start_date, end=end_date)\n",
        "\n",
        "            # Reset index for mplfinance compatibility\n",
        "            hourly_data.index = pd.to_datetime(hourly_data.index)\n",
        "\n",
        "            # Download subsequent data to see if it went up or not\n",
        "            # Ensure next day is a week day\n",
        "            # days_to_add = (7 - end_date.weekday()) % 7 not sure if this works properly\n",
        "\n",
        "            new_end = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=3)\n",
        "            new_end_date = new_end.strftime(\"%Y-%m-%d\")\n",
        "            subseq_data = yf.download(symbol, start=end_date, end=new_end_date)\n",
        "\n",
        "            # Define filename for the chart image\n",
        "            filename = f\"{symbol}_{start_date}_{end_date}.png\"\n",
        "\n",
        "            # Plot and save the candlestick chart\n",
        "            mpf.plot(hourly_data, type='candle', style='charles', volume=False,    # Disable volume\n",
        "                    ylabel='',      # Remove y-axis label\n",
        "                    xrotation=90,    # Rotate x-axis labels to be horizontal\n",
        "                    show_nontrading=False,   # Hide non-trading days\n",
        "                    savefig=dict(fname=filename, dpi=100, pad_inches=0.25))\n",
        "\n",
        "            # Determine the label based on price movement\n",
        "            try:\n",
        "                last_day_close = data['Close'].iloc[-1]\n",
        "                next_day_open = subseq_data.iloc[-1]['Close']\n",
        "\n",
        "                # 2 for increase, 0 for decrease, 1 for neither\n",
        "                difference = next_day_open - last_day_close\n",
        "\n",
        "                # 3% change tolerance\n",
        "                if difference >= 0.01*last_day_close:\n",
        "                    label = 2\n",
        "                elif difference <= -0.01*last_day_close:\n",
        "                    label = 0\n",
        "                else:\n",
        "                    label = 1\n",
        "\n",
        "                labels[filename] = label\n",
        "            except IndexError:  # Handle the case where there's no next day\n",
        "                # This might happen at the end of the dataset\n",
        "                labels[filename] = None\n",
        "\n",
        "\n",
        "                        # print the first graph just to see\n",
        "            if p:\n",
        "              mpf.plot(hourly_data, type='candle', style='charles', volume=False, ylabel='', xrotation=0, show_nontrading=False)\n",
        "              plt.show()\n",
        "              print(f\"file: {filename} \\t label: {labels[filename]}\")\n",
        "              p = False\n",
        "\n",
        "    print(f\"length of labels dict: {len(labels)}\")\n",
        "    print(f\"labels: {labels}\")\n",
        "    return labels\n",
        "\n",
        "def random_date(start, end):\n",
        "    \"\"\"Generate a random 10-day date between start and end dates.\"\"\"\n",
        "    start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
        "    end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
        "\n",
        "    # Ensure that the end date is at least 10 days after the start date\n",
        "    if end_date - start_date < timedelta(days=10):\n",
        "        raise ValueError(\"End date must be at least 10 days after start date.\")\n",
        "\n",
        "    # Calculate the maximum possible start date within the 10-day range\n",
        "    max_start_date = end_date - timedelta(days=10)\n",
        "\n",
        "    # Generate a random start date within the maximum start date\n",
        "    start_date = start_date + timedelta(days=random.randint(0, (max_start_date - start_date).days))\n",
        "\n",
        "    # Calculate the end date as 10 days after the start date\n",
        "    end_date = start_date + timedelta(days=10)\n",
        "\n",
        "    return start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def next_business_day(date):\n",
        "  while date.weekday() >= 5:  # Saturday or Sunday\n",
        "      date += timedelta(days=1)\n",
        "  return date\n",
        "\n",
        "\n",
        "# Generate charts and labels with augmented data\n",
        "labels = download_and_generate_charts(stock_symbols, num_samples_per_stock=100)"
      ],
      "metadata": {
        "id": "EQ6eAPT0Lk8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Define image dimensions\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# Updated model definition for 3-class classification with integer labels\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # Output layer for 3 classes\n",
        "])\n",
        "\n",
        "# Compile the model with sparse_categorical_crossentropy\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# List to store model performance metrics\n",
        "model_metrics = []\n",
        "\n",
        "# Loop through each image and its label\n",
        "for filename, label in labels.items():\n",
        "    try:\n",
        "        # Load image and preprocess\n",
        "        img = load_img(filename, target_size=(img_width, img_height))\n",
        "        img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "        img_array = np.expand_dims(img_array, axis=0)  # Adjust dimensions\n",
        "\n",
        "        # Use the same data for training and validation for simplicity\n",
        "        X_train, X_val = img_array, img_array\n",
        "        y_train, y_val = np.array([label]), np.array([label])\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "        # Evaluate the model\n",
        "        _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "        model_metrics.append((filename, accuracy))\n",
        "\n",
        "        print(f\"Model for {filename}: Accuracy = {accuracy}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "\n",
        "avg_accuracy = sum(metric[1] for metric in model_metrics)/len(model_metrics)\n",
        "print(f\"average accuracy: {avg_accuracy}\")"
      ],
      "metadata": {
        "id": "RYZo6j4PLl0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to change date ranges to be smaller and split training and testing sets and maybe add a tolerance for price changes so low changes don't matter"
      ],
      "metadata": {
        "id": "dZyOuNRGto6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Model with Separate Training and Validation Datasets"
      ],
      "metadata": {
        "id": "zmeZ3PoadOTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Define image dimensions\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# Updated model definition for 3-class classification with integer labels\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # Output layer for 3 classes\n",
        "])\n",
        "\n",
        "# Compile the model with sparse_categorical_crossentropy\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# List to store model performance metrics\n",
        "model_metrics = []\n",
        "\n",
        "# Convert labels dictionary to list of tuples (filename, label)\n",
        "file_label_pairs = list(labels.items())\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_pairs, val_pairs = train_test_split(file_label_pairs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Extract filenames and labels for training and validation sets\n",
        "train_filenames, train_labels = zip(*train_pairs)\n",
        "val_filenames, val_labels = zip(*val_pairs)\n",
        "\n",
        "# Lists to store image arrays\n",
        "train_images = []\n",
        "val_images = []\n",
        "\n",
        "# Load and preprocess images for training set\n",
        "for filename in train_filenames:\n",
        "    img = load_img(filename, target_size=(img_width, img_height))\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "    train_images.append(img_array)\n",
        "\n",
        "# Load and preprocess images for validation set\n",
        "for filename in val_filenames:\n",
        "    img = load_img(filename, target_size=(img_width, img_height))\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "    val_images.append(img_array)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train = np.array(train_images)\n",
        "X_val = np.array(val_images)\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation accuracy: {val_accuracy}\")\n"
      ],
      "metadata": {
        "id": "LzT-nUpGdLVG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}